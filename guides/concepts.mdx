---
title: "Core Concepts"
description: "Understanding Nebula's architecture: memories, chunks, and the vector graph"
---

## Memories and Chunks

In Nebula, everything is stored as a **memory**. A memory is a container with a unique `memory_id` that holds one or more **chunks**:

- **Document Memory**: An entire document, automatically split into chunks for processing. You can upload text, pre-chunked text, or a file. A short piece of text (like a user preference) is a document memory with a single chunk.
- **Conversation Memory**: A full conversation where each message is a chunk. Passing a `role` parameter tells Nebula to treat the memory as a conversation.

Each chunk has a unique **chunk ID** for granular updates or deletes.

## Document Memories

<CodeGroup>

```python Python
from nebula import Nebula, Memory

nebula = Nebula()
collection = nebula.create_collection(name="research_papers")

# Store document text
doc_id = nebula.create_document_text(
    collection_id=collection.id,
    raw_text="Full document content...",
    metadata={"title": "Research Paper"}
)

# Or upload a file
doc_id = nebula.store_memory(
    Memory.from_file("document.pdf", collection_id=collection.id, metadata={"title": "Research Paper"})
)

# Or store a short piece of text
nebula.store_memory(
    Memory(collection_id=collection.id, content="User prefers dark mode", metadata={"user_id": "user_789"})
)
```

```javascript JavaScript
import Nebula, { Memory } from '@nebula-ai/sdk';

const nebula = new Nebula({ apiKey: process.env.NEBULA_API_KEY });
const collection = await nebula.createCollection({ name: 'research_papers' });

// Store document text
const docId = await nebula.storeMemory({
  collection_id: collection.id,
  content: 'Full document content...',
  metadata: { title: 'Research Paper' }
});

// Upload a file (Node.js only)
const fileDocId = await nebula.storeMemory(
  await Memory.fromFile('document.pdf', collection.id, { title: 'Research Paper' })
);

// Or store a short piece of text
await nebula.storeMemory({
  collection_id: collection.id,
  content: 'User prefers dark mode',
  metadata: { user_id: 'user_789' }
});
```

```bash cURL
# Store document text
curl -X POST "https://api.trynebula.ai/v1/memories" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "collection_id": "COLLECTION_ID",
    "raw_text": "Full document content...",
    "metadata": {"title": "Research Paper"}
  }'

# Upload file (base64 encoded)
curl -X POST "https://api.trynebula.ai/v1/memories" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "collection_id": "COLLECTION_ID",
    "content_parts": [{
      "type": "document",
      "data": "BASE64_ENCODED_FILE_DATA",
      "media_type": "application/pdf",
      "filename": "document.pdf"
    }],
    "metadata": {"title": "Research Paper"}
  }'
```

</CodeGroup>

## Conversation Memories

Create a conversation by passing a `role`. Append messages by calling `store_memory()` with the same `memory_id`.

<CodeGroup>

```python Python
collection = nebula.create_collection(name="support_chats")

# Create a conversation memory
conv_id = nebula.store_memory(
    Memory(collection_id=collection.id, content="Hello! How can I help?", role="assistant")
)

# Add messages to the same conversation
nebula.store_memory(
    Memory(
        memory_id=conv_id,
        collection_id=collection.id,
        content=[
            {"content": "I need help", "role": "user"},
            {"content": "I'll help you", "role": "assistant"}
        ]
    )
)
```

```javascript JavaScript
const collection = await nebula.createCollection({ name: 'support_chats' });

// Create a conversation memory
const convId = await nebula.storeMemory({
  collection_id: collection.id,
  content: 'Hello! How can I help?',
  role: 'assistant'
});

// Add messages to the same conversation
await nebula.storeMemory({
  memory_id: convId,
  collection_id: collection.id,
  content: [
    { content: 'I need help', role: 'user' },
    { content: 'I\'ll help you', role: 'assistant' }
  ]
});
```

```bash cURL
# Create conversation
curl -X POST "https://api.trynebula.ai/v1/memories" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "collection_id": "COLLECTION_ID",
    "engram_type": "conversation",
    "messages": [{"content": "Hello! How can I help?", "role": "assistant"}]
  }'

# Add messages (use memory_id from response)
curl -X POST "https://api.trynebula.ai/v1/memories/MEMORY_ID/append" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "collection_id": "COLLECTION_ID",
    "messages": [
      {"content": "I need help", "role": "user"},
      {"content": "I'\''ll help you", "role": "assistant"}
    ]
  }'
```

</CodeGroup>

See [Conversations Guide](/guides/conversations) for multi-turn patterns.

## The Vector Graph

When you store memories, Nebula automatically extracts structured knowledge and builds a graph of entities and relationships. When you search, the response contains three layers:

- **Entities**: People, concepts, organizations, and other things extracted from your data, each with a name, description, and category
- **Facts**: Subject-predicate-value assertions about entities (e.g., "Sarah - led - the Aurora migration"), with confidence that grows through corroboration across sources
- **Utterances**: The original source text that grounds each entity and fact, providing provenance back to the stored memory

<Warning>The use of `metadata` is highly discouraged. Nebula already consolidates all the knowledge from your content automatically.</Warning>

## Memory Lifecycle

1. **Creation**: `store_memory()` without `memory_id` creates a new memory
2. **Expansion**: `store_memory()` with `memory_id` adds content to an existing memory
3. **Extraction**: Nebula extracts entities, facts, and relationships into the vector graph
4. **Retrieval**: `search()` returns entities, facts, and utterances; `get_memory()` returns the raw memory with all chunks
5. **Chunk Operations**: Use chunk IDs for granular updates or deletes
6. **Deletion**: `delete()` removes an entire memory and all its chunks

<Tip>Build complete units by using `memory_id` to group entire conversations or documents in one container, and group related memories into [collections](/guides/collections).</Tip>

## Next Steps

- [Memory Operations](/guides/memory-operations) - Store, retrieve, and delete memories
- [Collections](/guides/collections) - Organize memories into collections
- [Search](/guides/search) - Semantic search and filtering
