---
title: 'Memory Operations'
description: 'Store, retrieve, and manage memories in Nebula'
---

# Memory Operations

Core operations for working with memories: storing, retrieving, and deleting.

<Info>New to Nebula? Start with [Core Concepts](/guides/concepts) to understand the architecture.</Info>

## Storing Memories

<CodeGroup>

```python Python
from nebula import Nebula
nebula = Nebula(api_key="your-api-key")

# Single memory
memory_id = nebula.store_memory({
    "collection_id": "research",
    "content": "Machine learning automates model building",
    "metadata": {"topic": "AI"}
})

# Batch storage (more efficient)
memory_ids = nebula.store_memories([
    {"collection_id": "research", "content": "Neural networks...", "metadata": {"type": "concept"}},
    {"collection_id": "research", "content": "Deep learning...", "metadata": {"type": "concept"}}
])
```

```javascript JavaScript
const { Nebula } = require('@nebula-ai/sdk');
const nebula = new Nebula({ apiKey: 'your-api-key' });

// Single memory
const memoryId = await nebula.storeMemory({
  collection_id: "research",
  content: "Machine learning automates model building",
  metadata: { topic: "AI" }
});

// Batch storage (more efficient)
const memoryIds = await nebula.storeMemories([
  { collection_id: "research", content: "Neural networks...", metadata: { type: "concept" } },
  { collection_id: "research", content: "Deep learning...", metadata: { type: "concept" } }
]);
```

```bash cURL
curl -X POST "https://api.trynebula.ai/v1/memories" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "collection_id": "COLLECTION_ID",
    "engram_type": "document",
    "raw_text": "Machine learning automates model building",
    "metadata": {"topic": "AI"}
  }'
```

</CodeGroup>

### Conversation Messages

<CodeGroup>

```python Python
# Create conversation
conv_id = nebula.store_memory({
    "collection_id": "support",
    "content": "Hello! How can I help?",
    "role": "assistant"
})

# Add to same conversation
nebula.store_memory({
    "memory_id": conv_id,
    "collection_id": "support",
    "content": "I need help with my account",
    "role": "user"
})
```

```javascript JavaScript
const convId = await nebula.storeMemory({
  collection_id: "support",
  content: "Hello! How can I help?",
  role: "assistant"
});

await nebula.storeMemory({
  memory_id: convId,
  collection_id: "support",
  content: "I need help with my account",
  role: "user"
});
```

```bash cURL
curl -X POST "https://api.trynebula.ai/v1/memories" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "collection_id": "COLLECTION_ID",
    "engram_type": "conversation",
    "messages": [
      {"role": "assistant", "content": "Hello! How can I help?"},
      {"role": "user", "content": "I need help with my account"}
    ]
  }'
```

</CodeGroup>

See [Conversations Guide](/guides/conversations) for multi-turn patterns.

### Document Upload

Upload a document as raw text, pre-chunked text, or a file. File processing (OCR/transcription/text extraction) happens automatically.

<CodeGroup>

```python Python
from nebula import Nebula, Memory

nebula = Nebula(api_key="your-api-key")
collection = nebula.get_collection_by_name("my-collection")

# Upload text
doc_id = nebula.create_document_text(
    collection_id=collection.id,
    raw_text="Machine learning is a subset of AI...",
    metadata={"title": "ML Intro"}
)

# Upload pre-chunked
doc_id = nebula.create_document_chunks(
    collection_id=collection.id,
    chunks=["Chapter 1...", "Chapter 2..."],
    metadata={"title": "My Doc"}
)

# Upload a file
doc_id = nebula.store_memory(
    Memory.from_file("document.pdf", collection_id=collection.id, metadata={"title": "Research Paper"})
)
```

```javascript JavaScript
import Nebula, { Memory } from '@nebula-ai/sdk';

const nebula = new Nebula({ apiKey: 'your-api-key' });
const collection = await nebula.getCollectionByName('my-collection');

// Upload text
const docId = await nebula.storeMemory({
  collection_id: collection.id,
  content: "Machine learning is a subset of AI...",
  metadata: { title: "ML Intro" }
});

// Upload pre-chunked
const chunkedDocId = await nebula.storeMemory({
  collection_id: collection.id,
  content: ["Chapter 1...", "Chapter 2..."],
  metadata: { title: "My Doc" }
});

// Upload a file (Node.js only)
const fileDocId = await nebula.storeMemory(
  await Memory.fromFile('document.pdf', collection.id, { title: 'Research Paper' })
);
```

```bash cURL
# Upload file (base64 encoded)
curl -X POST "https://api.trynebula.ai/v1/memories" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "collection_id": "COLLECTION_ID",
    "content_parts": [{
      "type": "document",
      "data": "BASE64_ENCODED_FILE_DATA",
      "media_type": "application/pdf",
      "filename": "document.pdf"
    }],
    "metadata": {"title": "Research Paper"}
  }'
```

</CodeGroup>

<Tip>Inline base64 uploads are limited to ~5MB per file part; larger files use a presigned upload flow (max 100MB).</Tip>

## Retrieving Memories

<CodeGroup>

```python Python
# Get by ID
memory = nebula.get_memory(memory_id)

# List memories in collection
memories = nebula.list_memories(collection_ids=["research"], limit=50)
```

```javascript JavaScript
// Get by ID
const memory = await nebula.getMemory(memoryId);

// List memories in collection
const memories = await nebula.listMemories({ collection_ids: ["research"], limit: 50 });
```

```bash cURL
# Get by ID
curl -X GET "https://api.trynebula.ai/v1/engrams/YOUR_MEMORY_ID" \
  -H "Authorization: Bearer YOUR_API_KEY"

# List memories
curl -X GET "https://api.trynebula.ai/v1/engrams?limit=50" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

</CodeGroup>

<Info>For semantic search (finding by meaning), see the [Search Guide](/guides/search).</Info>

## Deleting Memories

<CodeGroup>

```python Python
# Delete single
nebula.delete(memory_id)

# Delete multiple
nebula.delete([memory_id_1, memory_id_2, memory_id_3])
```

```javascript JavaScript
await nebula.delete(memoryId);
await nebula.delete([memoryId1, memoryId2, memoryId3]);
```

```bash cURL
curl -X DELETE "https://api.trynebula.ai/v1/engrams/YOUR_MEMORY_ID" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

</CodeGroup>

<Warning>Deletion is permanent and cannot be undone.</Warning>

## Bulk Operations

Use `list_memories()` with `metadata_filters` to target a set, then delete or update metadata in batches.

<CodeGroup>

```python Python
memories = nebula.list_memories(
    collection_ids=["docs"],
    metadata_filters={"metadata.status": {"$eq": "archived"}},
    limit=1000
)

# Bulk delete
nebula.delete([m.id for m in memories])

# Bulk metadata update
for m in memories:
    nebula.update_memory(
        memory_id=m.id,
        metadata={"archived": True},
        merge_metadata=True
    )
```

```javascript JavaScript
const memories = await nebula.listMemories({
  collection_ids: ["docs"],
  metadata_filters: {"metadata.status": {"$eq": "archived"}},
  limit: 1000
});

// Bulk delete
await nebula.delete(memories.map(m => m.id));

// Bulk metadata update
for (const m of memories) {
  await nebula.updateMemory({
    memoryId: m.id,
    metadata: { archived: true },
    mergeMetadata: true
  });
}
```

```bash cURL
curl -G "https://api.trynebula.ai/v1/memories" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  --data-urlencode 'collection_ids=docs' \
  --data-urlencode 'limit=1000' \
  --data-urlencode 'metadata_filters={"metadata.status":{"$eq":"archived"}}'

curl -X POST "https://api.trynebula.ai/v1/memories/delete" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"ids": ["MEMORY_ID_1", "MEMORY_ID_2"]}'
```

</CodeGroup>

<Info>Use `store_memory()` to append content. `update_memory()` only updates name, metadata, or collection associations.</Info>

## Chunk Operations

Memories contain chunks (messages in conversations, sections in documents). Each chunk has a unique ID.

<CodeGroup>

```python Python
memory = nebula.get_memory(memory_id)
for chunk in memory.chunks:
    print(f"Chunk {chunk.id}: {chunk.content[:50]}...")

# Delete specific chunk
nebula.delete_chunk(chunk_id)

# Update chunk
nebula.update_chunk(chunk_id=chunk_id, content="Updated content", metadata={"edited": True})
```

```javascript JavaScript
const memory = await nebula.getMemory(memoryId);
memory.chunks?.forEach(chunk => console.log(`Chunk ${chunk.id}: ${chunk.content}`));

await nebula.deleteChunk(chunkId);
await nebula.updateChunk(chunkId, "Updated content", { edited: true });
```

```bash cURL
# Get memory with chunks
curl -X GET "https://api.trynebula.ai/v1/engrams/YOUR_MEMORY_ID" \
  -H "Authorization: Bearer YOUR_API_KEY"

# Delete chunk
curl -X DELETE "https://api.trynebula.ai/v1/chunks/YOUR_CHUNK_ID" \
  -H "Authorization: Bearer YOUR_API_KEY"

# Update chunk
curl -X POST "https://api.trynebula.ai/v1/chunks/YOUR_CHUNK_ID" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{"id": "YOUR_CHUNK_ID", "text": "Updated content", "metadata": {"edited": true}}'
```

</CodeGroup>

## Best Practices

1. **Use batch operations** - `store_memories()` is faster than multiple single calls
2. **Add descriptive metadata** - Makes filtering and organization easier
3. **Organize by collections** - Group related memories logically
4. **Handle errors gracefully** - Use try/catch for API operations

<Tip>Rich metadata improves search quality. See [Metadata Filtering](/guides/metadata-filtering) for filter operators.</Tip>

## Next Steps

- [Search Guide](/guides/search) - Semantic search and filtering
- [Collections Guide](/guides/collections) - Organize memories into collections
