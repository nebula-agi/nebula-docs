---
title: 'Memory Operations'
description: 'Store, retrieve, and manage memories in Nebula'
---

# Memory Operations

Learn the core operations for working with memories in Nebula: storing, retrieving, and deleting.

<Info>New to Nebula? Start with [Core Concepts](/guides/concepts) to understand the architecture.</Info>

## Storing Memories

### Single Memory

<CodeGroup>

```python Python
from nebula_client import Nebula

nebula = Nebula(api_key="your-api-key")

memory_id = nebula.store_memory({
    "collection_id": "research-collection",
    "content": "Machine learning automates analytical model building",
    "metadata": {"topic": "AI", "difficulty": "intermediate"}
})

print(f"Stored memory: {memory_id}")
```

```javascript JavaScript
const { Nebula } = require('@nebula-ai/sdk');
const nebula = new Nebula({ apiKey: 'your-api-key' });

const memoryId = await nebula.storeMemory({
  collection_id: "research-collection",
  content: "Machine learning automates analytical model building",
  metadata: { topic: "AI", difficulty: "intermediate" }
});

console.log(`Stored memory: ${memoryId}`);
```

</CodeGroup>

**Returns**: Memory ID - use this to reference the memory later.

### Batch Storage

Store multiple memories at once for better performance:

<CodeGroup>

```python Python
memories = [
    {
        "collection_id": "research-collection",
        "content": "Supervised learning uses labeled training data",
        "metadata": {"type": "definition"}
    },
    {
        "collection_id": "research-collection",
        "content": "Neural networks are inspired by biological systems",
        "metadata": {"type": "concept"}
    }
]

memory_ids = nebula.store_memories(memories)
print(f"Stored {len(memory_ids)} memories")
```

```javascript JavaScript
const memories = [
  {
    collection_id: "research-collection",
    content: "Supervised learning uses labeled training data",
    metadata: { type: "definition" }
  },
  {
    collection_id: "research-collection",
    content: "Neural networks are inspired by biological systems",
    metadata: { type: "concept" }
  }
];

const memoryIds = await nebula.storeMemories(memories);
console.log(`Stored ${memoryIds.length} memories`);
```

</CodeGroup>

<Tip>Batch operations are more efficient than storing memories one at a time.</Tip>

### Conversation Messages

Store conversational exchanges with roles:

<CodeGroup>

```python Python
# Initial message
conversation_id = nebula.store_memory({
    "collection_id": "support-collection",
    "content": "Hello! How can I help you?",
    "role": "assistant",
    "metadata": {"session_id": "session_123"}
})

# Follow-up messages
nebula.store_memory({
    "memory_id": conversation_id,  # Add to same conversation
    "collection_id": "support-collection",
    "content": "I need help with my account",
    "role": "user"
})
```

```javascript JavaScript
// Initial message
const conversationId = await nebula.storeMemory({
  collection_id: "support-collection",
  content: "Hello! How can I help you?",
  role: "assistant",
  metadata: { session_id: "session_123" }
});

// Follow-up messages
await nebula.storeMemory({
  memory_id: conversationId,  // Add to same conversation
  collection_id: "support-collection",
  content: "I need help with my account",
  role: "user"
});
```

</CodeGroup>

See [Conversations Guide](/guides/conversations) for multi-turn conversation patterns.

## Document Upload

<CodeGroup>

```python Python
# Upload text
doc_id = nebula.create_document_text(
    collection_ref="my-collection",
    raw_text="Machine learning is a subset of AI...",
    metadata={"title": "ML Intro"}
)

# Upload from file
with open("doc.txt", "r") as f:
    doc_id = nebula.create_document_text(
        collection_ref="my-collection",
        raw_text=f.read(),
        metadata={"filename": "doc.txt"}
    )

# Upload pre-chunked
doc_id = nebula.create_document_chunks(
    collection_ref="my-collection",
    chunks=["Intro...", "Chapter 1...", "Chapter 2..."],
    metadata={"title": "My Doc"}
)
```

```javascript JavaScript
// Upload text
const docId = await nebula.createDocumentText({
  collection_ref: "my-collection",
  raw_text: "Machine learning is a subset of AI...",
  metadata: { title: "ML Intro" }
});

// Upload from file
const fs = require('fs').promises;
const content = await fs.readFile('doc.txt', 'utf-8');
const docId = await nebula.createDocumentText({
  collection_ref: "my-collection",
  raw_text: content,
  metadata: { filename: "doc.txt" }
});

// Upload pre-chunked
const docId = await nebula.createDocumentChunks({
  collection_ref: "my-collection",
  chunks: ["Intro...", "Chapter 1...", "Chapter 2..."],
  metadata: { title: "My Doc" }
});
```

</CodeGroup>

**Ingestion modes**: `fast` (default), `hi-res` (better quality), `custom`

**Web UI**: Click "Add Memory" → "Upload File" → Select .txt/.md/.json/.csv/.log file

## Retrieving Memories

### Get by ID

<CodeGroup>

```python Python
memory = nebula.get_memory(memory_id)

print(f"Content: {memory.content}")
print(f"Metadata: {memory.metadata}")
print(f"Created: {memory.created_at}")
```

```javascript JavaScript
const memory = await nebula.getMemory(memoryId);

console.log(`Content: ${memory.content}`);
console.log(`Metadata: ${memory.metadata}`);
console.log(`Created: ${memory.created_at}`);
```

</CodeGroup>

### List Memories

Retrieve all memories in a collection:

<CodeGroup>

```python Python
memories = nebula.list_memories(
    collection_ids=["research-collection"],
    limit=50,
    offset=0
)

for memory in memories:
    print(f"{memory.content[:50]}...")
```

```javascript JavaScript
const memories = await nebula.listMemories({
  collection_ids: ["research-collection"],
  limit: 50,
  offset: 0
});

memories.forEach(memory => {
  console.log(`${memory.content?.substring(0, 50)}...`);
});
```

</CodeGroup>

<Info>For semantic search (finding memories by meaning), see the [Search Guide](/guides/search).</Info>

## Deleting Memories

### Delete Single Memory

<CodeGroup>

```python Python
result = nebula.delete(memory_id)
print(f"Deleted: {result}")
```

```javascript JavaScript
const result = await nebula.delete(memoryId);
console.log(`Deleted: ${result}`);
```

</CodeGroup>

### Batch Delete

<CodeGroup>

```python Python
# Delete multiple memories
result = nebula.delete([memory_id_1, memory_id_2, memory_id_3])
print(f"Deleted {result['deleted_count']} memories")
```

```javascript JavaScript
// Delete multiple memories
const result = await nebula.delete([memoryId1, memoryId2, memoryId3]);
console.log(`Deleted ${result.deleted_count} memories`);
```

</CodeGroup>

<Warning>Deletion is permanent and cannot be undone.</Warning>

## Chunk Operations

Memories can contain multiple chunks (for documents) or messages (for conversations). Each chunk has a unique ID for granular operations.

### Get Chunks

When retrieving a memory, chunks include their IDs:

<CodeGroup>

```python Python
memory = nebula.get_memory(memory_id)

# Access chunks with IDs
for chunk in memory.chunks:
    print(f"Chunk {chunk.id}: {chunk.content[:50]}...")
    if chunk.role:  # For conversation messages
        print(f"Role: {chunk.role}")
```

```javascript JavaScript
const memory = await nebula.getMemory(memoryId);

// Access chunks with IDs
memory.chunks?.forEach(chunk => {
  console.log(`Chunk ${chunk.id}: ${chunk.content.substring(0, 50)}...`);
  if (chunk.role) {  // For conversation messages
    console.log(`Role: ${chunk.role}`);
  }
});
```

</CodeGroup>

### Delete Chunk

Delete a specific chunk or message:

<CodeGroup>

```python Python
# Delete a specific message in a conversation
nebula.delete_chunk(chunk_id)
```

```javascript JavaScript
// Delete a specific message in a conversation
await nebula.deleteChunk(chunkId);
```

</CodeGroup>

### Update Chunk

Update content or metadata of a specific chunk:

<CodeGroup>

```python Python
# Update chunk content
nebula.update_chunk(
    chunk_id=chunk_id,
    content="Updated content here",
    metadata={"edited": True}
)
```

```javascript JavaScript
// Update chunk content
await nebula.updateChunk(
  chunkId,
  "Updated content here",
  { edited: true }
);
```

</CodeGroup>

<Info>Chunk IDs are automatically generated when storing memories. You only need them for granular operations.</Info>

## Complete Example

Here's a full workflow demonstrating all operations:

<CodeGroup>

```python Python
from nebula_client import Nebula

nebula = Nebula(api_key="your-api-key")

# 1. Create collection
collection = nebula.create_collection(name="demo", description="Example collection")

# 2. Store memories
memories = [
    {
        "collection_id": collection.id,
        "content": "Python is a high-level programming language",
        "metadata": {"language": "python"}
    },
    {
        "collection_id": collection.id,
        "content": "JavaScript is used for web development",
        "metadata": {"language": "javascript"}
    }
]
memory_ids = nebula.store_memories(memories)
print(f"Stored {len(memory_ids)} memories")

# 3. Retrieve
memory = nebula.get_memory(memory_ids[0])
print(f"Retrieved: {memory.content}")

# 4. Search
results = nebula.search(
    query="programming languages",
    collection_ids=[collection.id],
    limit=5
)
print(f"Found {len(results)} results")

# 5. Delete
nebula.delete(memory_ids)
print("Deleted all memories")
```

```javascript JavaScript
const { Nebula } = require('@nebula-ai/sdk');
const nebula = new Nebula({ apiKey: 'your-api-key' });

async function fullWorkflow() {
  // 1. Create collection
  const collection = await nebula.createCollection({
    name: "demo",
    description: "Example collection"
  });

  // 2. Store memories
  const memories = [
    {
      collection_id: collection.id,
      content: "Python is a high-level programming language",
      metadata: { language: "python" }
    },
    {
      collection_id: collection.id,
      content: "JavaScript is used for web development",
      metadata: { language: "javascript" }
    }
  ];

  const memoryIds = await nebula.storeMemories(memories);
  console.log(`Stored ${memoryIds.length} memories`);

  // 3. Retrieve
  const memory = await nebula.getMemory(memoryIds[0]);
  console.log(`Retrieved: ${memory.content}`);

  // 4. Search
  const results = await nebula.search({
    query: "programming languages",
    collection_ids: [collection.id],
    limit: 5
  });
  console.log(`Found ${results.length} results`);

  // 5. Delete
  await nebula.delete(memoryIds);
  console.log("Deleted all memories");
}

fullWorkflow();
```

</CodeGroup>

## Best Practices

1. **Use batch operations** - `store_memories()` is faster than multiple `store_memory()` calls
2. **Add descriptive metadata** - Makes filtering and organization easier
3. **Organize by collections** - Group related memories for better access control
4. **Validate before storing** - Check that required fields (`collection_id`, `content`) are present
5. **Handle errors gracefully** - Use try/catch for API operations

<Tip>Rich metadata improves search quality. Include fields like `type`, `category`, `priority`, or `tags`.</Tip>

## Metadata Best Practices

Good metadata structure:

```python
{
    "collection_id": "support-collection",
    "content": "User reported login issue",
    "metadata": {
        "type": "support_ticket",
        "priority": "high",
        "user_id": "user_123",
        "category": "authentication",
        "tags": ["login", "bug"],
        "created_by": "agent_001"
    }
}
```

This enables powerful filtering:
- Find all high-priority tickets
- Search tickets for specific users
- Filter by category or tags

Learn more in [Metadata Filtering](/guides/metadata-filtering).

## Next Steps

- [Search Guide](/guides/search) - Semantic search and filtering
- [Collections Guide](/guides/collections) - Organize memories into collections
- [Conversations](/guides/conversations) - Build chat applications
- [Metadata Filtering](/guides/metadata-filtering) - Advanced filtering
