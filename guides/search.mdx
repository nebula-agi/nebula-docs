---
title: 'Search Guide'
description: 'Vector graph search in Nebula'
---

Nebula searches over a vector graph of entities and relationships, not raw text chunks. You send a natural language query; Nebula traverses the graph and returns structured knowledge: **entities** (who and what), **facts** (assertions about them), and **utterances** (the original source text).

## Basic Search

<CodeGroup>

```python Python
from nebula import Nebula, Memory

nebula = Nebula()

results = nebula.search(
    query="What has Sarah worked on?",
    collection_ids=["engineering"]
)

# Entities: people, concepts, organizations
for entity in results.entities:
    print(f"{entity['name']} ({entity.get('category', 'unknown')})")
    print(f"  Score: {entity['activation_score']:.2f}")

# Facts: structured subject-predicate-value triples
for fact in results.facts:
    print(f"{fact['subject']} → {fact['predicate']} → {fact['value']}")

# Utterances: original source text grounding the facts
for utterance in results.utterances:
    print(f"[{utterance.get('speaker', 'unknown')}]: {utterance['text']}")
```

```javascript JavaScript
const { Nebula } = require('@nebula-ai/sdk');
const nebula = new Nebula({ apiKey: process.env.NEBULA_API_KEY });

const results = await nebula.search({
  query: "What has Sarah worked on?",
  collection_ids: ["engineering"]
});

// Entities
results.entities.forEach(e =>
  console.log(`${e.name} (${e.category ?? 'unknown'}) - score: ${e.activation_score}`)
);

// Facts
results.facts.forEach(f =>
  console.log(`${f.subject} → ${f.predicate} → ${f.value}`)
);

// Utterances
results.utterances.forEach(u =>
  console.log(`[${u.speaker ?? u.source_role ?? 'unknown'}]: ${u.text}`)
);
```

```bash cURL
curl -X POST "https://api.trynebula.ai/v1/memories/search" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "query": "What has Sarah worked on?",
    "collection_ids": ["engineering"]
  }'
```

</CodeGroup>

## Understanding Results

Search returns a `MemoryResponse` with three layers of knowledge:

| Layer | Contains | Example |
|-------|----------|---------|
| **Entities** | People, concepts, organizations with profiles | `Sarah Chen` (person, score: 0.94) |
| **Facts** | Subject-predicate-value triples | `Sarah Chen → led → Aurora migration` |
| **Utterances** | Original source text that grounds the facts | `"Sarah led the migration from PostgreSQL to Aurora last quarter"` |

Each item has an `activation_score` (0-1) reflecting its relevance to the query. Facts link back to their source utterances, and entities link to their facts - giving you a full provenance chain from structured knowledge down to original text.

<Info>The vector graph is built automatically when you store memories. You don't need to define entities or relationships - Nebula extracts them.</Info>

## Search Effort

The `effort` parameter is the primary control for search. It determines how deeply Nebula traverses the vector graph.

| Effort | Depth | Use Case |
|--------|-------|----------|
| `auto` | Adapts to query | Default - good for most queries |
| `low` | 2 hops, narrow | Fast lookups, simple factual queries |
| `medium` | 2 hops, wider | Broader exploration across more relationships |
| `high` | 3 hops, widest | Deep multi-hop reasoning across the graph |

<CodeGroup>

```python Python
# Quick factual lookup
results = nebula.search(query="Sarah's role", effort="low")

# Deep exploration
results = nebula.search(query="How are the Q4 projects connected?", effort="high")
```

```javascript JavaScript
// Quick factual lookup
const results = await nebula.search({ query: "Sarah's role", effort: "low" });

// Deep exploration
const deep = await nebula.search({ query: "How are the Q4 projects connected?", effort: "high" });
```

```bash cURL
curl -X POST "https://api.trynebula.ai/v1/memories/search" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "query": "How are the Q4 projects connected?",
    "effort": "high"
  }'
```

</CodeGroup>

<Tip>Start with the default (`auto`). Only increase effort if you need the graph to discover connections further from the query.</Tip>

## Scoping with Collections

Use `collection_ids` to scope which part of the vector graph is searched.

<CodeGroup>

```python Python
# Search specific collections
results = nebula.search(
    query="customer feedback",
    collection_ids=["support-2024", "support-2023", "surveys"]
)

# Search all accessible collections (omit collection_ids)
results = nebula.search(query="product roadmap")
```

```javascript JavaScript
const results = await nebula.search({
  query: "customer feedback",
  collection_ids: ["support-2024", "support-2023", "surveys"]
});

// Search all accessible collections
const all = await nebula.search({ query: "product roadmap" });
```

```bash cURL
curl -X POST "https://api.trynebula.ai/v1/memories/search" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "query": "customer feedback",
    "collection_ids": ["support-2024", "support-2023", "surveys"]
  }'
```

</CodeGroup>

<Warning>Searching all collections is slower than targeting specific ones.</Warning>

## Authority Scores

Authority is a store-time parameter on conversation messages that tells Nebula how much to trust a piece of content. It's stored per-message, not per-memory.

```python
# Verified answer from support agent
conv_id = nebula.store_memory(
    Memory(
        collection_id="support",
        content="Here is the verified answer...",
        role="assistant",
        authority=0.9
    )
)

# Uncertain user message
nebula.store_memory(
    Memory(
        memory_id=conv_id,
        collection_id="support",
        content="I'm not sure, maybe try this?",
        role="user",
        authority=0.4
    )
)
```

| Score | Use Case |
|-------|----------|
| `0.9-1.0` | Verified facts, official sources |
| `0.5-0.7` | General content (default: 0.5) |
| `0.0-0.3` | Low confidence or uncertain |

## Advanced Options

### Hybrid Search Weights

Control the balance between semantic and full-text matching for seed discovery (how the graph traversal finds its starting points):

<CodeGroup>

```python Python
results = nebula.search(
    query="neural networks",
    collection_ids=["research"],
    search_settings={
        "semantic_weight": 0.7,
        "fulltext_weight": 0.3
    }
)
```

```javascript JavaScript
const results = await nebula.search({
  query: "neural networks",
  collection_ids: ["research"],
  searchSettings: {
    semanticWeight: 0.7,
    fulltextWeight: 0.3
  }
});
```

```bash cURL
curl -X POST "https://api.trynebula.ai/v1/memories/search" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "query": "neural networks",
    "collection_ids": ["research"],
    "search_settings": {
      "semantic_weight": 0.7,
      "fulltext_weight": 0.3
    }
  }'
```

</CodeGroup>

Defaults (0.8 semantic, 0.2 fulltext) work well for most cases. Lower semantic weight if you need exact keyword matching.

### Metadata Filters

Optionally narrow the search scope using metadata constraints. Filters restrict which part of the graph is entered - the graph still handles discovery within that scope.

```python
results = nebula.search(
    query="project updates",
    collection_ids=["work"],
    filters={
        "status": "active",
        "team": "engineering"
    }
)
```

**Operators:** `$eq`, `$ne`, `$in`, `$nin`, `$gt`, `$gte`, `$lt`, `$lte`, `$like`, `$ilike`, `$overlap`, `$contains`, `$and`, `$or`

See [Metadata Filtering](/guides/metadata-filtering) for the full reference.

## Next Steps

- [Metadata Filtering](/guides/metadata-filtering) - Filter operator reference
- [Memory Operations](/guides/memory-operations) - Store and manage memories
